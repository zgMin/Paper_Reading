# Fast Inference from Transformers via Speculative Decoding

**作者**: Yaniv Leviathan, Matan Kalman, Yossi Matias

**出版年份**: 2023

**期刊/会议**: Proceedings of the 40th International Conference on Machine Learning (ICML-2023)

## 摘要
本论文介绍了投机解码，一种从自回归模型中更快地采样的算法，而不改变输出。作者提出了使用更高效的模型并行运行多个标记以近似更容易的子任务。该方法加速了大型自回归模型（如Transformers）的推断，无需重新训练或架构更改。作者演示了与T5X模型标准实现相比的2倍至3倍加速。

## 介绍
- 背景和动机：大型自回归模型，如Transformers，比较小型模型更有能力，但速度较慢。从这些较大模型中解码K个标记需要对模型进行K次串行运行。本论文讨论了大型自回归模型的重要性以及更快推断方法的需求。现有方法要么旨在减少所有输入的推断成本，要么使用更容易的推断步骤的自适应计算方法。然而，这些解决方案通常需要更改模型架构、训练过程，并且无法保持相同的输出。
- 方法：作者提出了投机解码作为一种补充方法，以增加并行性，而无需更改模型架构、训练过程或输出分布。

## 方法

<img width="808" alt="image" src="https://github.com/zgMin/Paper_Reading/assets/52092775/b8986b9e-3708-4042-875e-82f37268c090">


作者将常用于处理器的猜测执行技术引入了自回归模型解码的随机设置。猜测解码涉及并行从目标模型和近似模型中采样标记。接受概率是基于目标模型概率的最大值以及近似模型概率与目标模型概率比率的计算而确定的。作者提供了理论分析和证明，以支持猜测解码的正确性和效率。

下面是该方法的算法流程：

> 1. 输入Mp大模型，Mq小模型，和前缀prefix
> 2. 让小模型Mq生成r个文字（草稿），这里因为decode模型是一个一个字蹦的，需要执行r遍，并对自己产生的r个字打分q(x)
> 3. 把prefix和r个文字（草稿）加在一起让大模型看看，并让大模型打分p(x)
> 4. 每个字的分挨个对比，找到第n个字，n前面的字都符合大模型的打分
> 5. 让大模型生成第n个文字，叫做t
> 6. 返回 prefix+前n个文字+t

举个例子：

> prefix：【Q：请写一首李白的诗。A:】
> 小模型生成 ： 【好的，这是一首李白的诗歌：李白李白的的的】
> 大模型打分：【好的，这是一首李白的诗歌】 -> 这段写的没问题 (即p(x)/q(x)大于阈值)，【李白李白的的的】->这段写的什么玩意(即p(x)/q(x)小于阈值)
> 大模型生成后续t：【君不见黄河之水天上来】
> 最后结果为：prefix【Q：请写一首李白的诗。A:】+x[1..n]【好的，这是一首李白的诗歌：】+t【君不见黄河之水天上来】

并行计算(可跳过阅读)，从原理来说不需要并行计算的，论文里用并行是为了极致的加速，这里说下是怎么并行计算的。

> prefix：【Q：请写一首李白的诗。A:】
> 小模型生成 ： 【好的，李白李白的的的】 注意小模型是一个字一个字的蹦出来的

> 第1个字蹦出来时 大模型并行1打分：【Q：请写一首李白的诗。A:】+【好】 -> 这段写的没问题 ok 并得出下一个字t为【的】
> 第2个字蹦出来时 大模型并行2打分：【Q：请写一首李白的诗。A:】+【好的】 -> 这段写的没问题 ok 并得出下一个字t为【，】
> 第3个字蹦出来时 大模型并行3打分：【Q：请写一首李白的诗。A:】+【好的,】 -> 这段写的没问题 ok 并得出下一个字t为【君】
> 第4个字蹦出来时 大模型并行4打分：【Q：请写一首李白的诗。A:】+【好的,李】 -> 这个有问题
> 大模型4还没计算完时 第5个字蹦出来时 大模型并行5打分：【Q：请写一首李白的诗。A:】+【好的,李白】 -> 这个有问题

> 综合结果就会取3的输出结果直接当作t

> 最后结果为：prefix【Q：请写一首李白的诗。A:】+x[1..n]【好的，】+t【君】

> 然后继续重复上述步骤

论文里对Mp进行并行计算，是一种不顾及计算资源的加速。它在每一步都尝试并行计算大模型的观点，从而达到速度上的最优化，但同时就十分要求并行的计算能力，比如r为5时，就需要5个大模型同时计算。

在极致并行的情况下，速度可以达到理论最优，但代价是算力的浪费，这在工程上是不可接受的。

## 实验结果
作者在T5-XXL模型上评估了提出的方法，并将其与标准的T5X实现进行了比较。他们使用批量大小为1来测量在最大似然采样和标准采样下的墙时改进。经验结果显示，在翻译任务上速度提高了2.6倍至3.4倍，在摘要任务上提高了2.3倍至3.1倍。作者还测试了不同的近似模型，如T5-large、T5-base和T5-small，并观察到随着近似模型规模的增加，速度提升也增加了。

<img width="409" alt="image" src="https://github.com/zgMin/Paper_Reading/assets/52092775/e95f2542-c033-4ea6-8ea7-28a40532eacc">


## 讨论
作者讨论了理论预测与实际运行时间的匹配，并强调了猜测解码的优势，如更快的推断，而无需更改模型架构或训练过程。他们还提到了在大型模型中由于内存带宽和通信限制可能存在的额外计算资源的可用性。本论文提供了有关加速自回归模型推断的猜测解码的权衡和优势的见解。

## 结论
本论文提出了投机解码作为一种算法，可加速从大型自回归模型进行推断，而无需重新训练或更改架构。该方法允许使用更高效的模型并行运行多个标记，以处理更容易的子任务。实验结果显示与T5X模型标准实现相比，有显著的加速。作者强调了投机解码在各种应用中提高自回归模型效率的潜力。

## 个人观点
- 这项研究对于改善各个领域中自回归模型的效率具有重要意义，有望实现自然语言处理和其他领域中更快速、可伸缩的推断。
- 方法本质上与[Accelerating Large Language Model Decodingwith Speculative Sampling](Speculative_Decoding/Accelerating_Large_Language_Model_Decoding_with_Speculative_Sampling.md)没有区别,多了一些证明
- 这种方法就很适合Code Lamma，因为代码生成包含很多样板话。就如开发者Georgi Gerganov在例子中用的就是Code Lamma。相反，一些非样板话的文字，或者小模型和大模型生成差距较大的文字，比如诗歌，就很难通过这种方式加速。
